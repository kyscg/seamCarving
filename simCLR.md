Contrastive learning involves four major ideas. Firstly, we augment the data by performing various transforms in order. Then we obtain representation vectors using a neural network and then transform it to a space where we can perform contrastive learning. In the paper, the authors have used random cropping, and random color distortions along with Gaussian blur while mentioning that the former two turn out to be crucial for learning. For getting the representation vectors, the authors use a ResNet and a simple MLP with one hidden layer. Finally, we compute pairwise cosine similarity and use a normalized temperature-scaled cross entropy loss to train the model.

One thing that I found interesting from this paper was how the authors provide an explanation about why traditional methods of data-augmentation like cropping and resizing don't help us understand other forms of augmentation. To circumvent this, they composite data augmentation by cropping and resizing tot he same resolution and then applying augmentations to only one branch of the data. The authors provide a histogram of colours that show why using only random cropping doesn't work, as the color distributions turn out to be the same. One way to understand is that the model is learning representations of objects in a colour-agnostic manner, which is closer to our ideal scenario of achieving models that can generalize well.

Other contributions of the paper are related to details about contrastive learning. Firstly, contrastive learning needs stronger data augmentation, this is related to the findings from the previous paragraph where they compared the usual random cropping to composite augmentations. Another finding is that normalization and temperature scaling helps contrastive learning.  The nonlinear layer is very useful as well, it introduces nonlinearity to the representations learnt and separates generic representations from contrastive loss representations. Finally, the authors point out that contrastive learning performs better with larger batch sizes and longer training time, but I failed to understand the reasoning that they give for this.

Overall, I think this paper made a good step in the direction of generalizing across domains by using stronger data-augmentation methods and contrastive learning. The temperature scaling is an interesting idea, and to me, it feels like it varies the distribution depending on the value and thereby improves contrastive learning.